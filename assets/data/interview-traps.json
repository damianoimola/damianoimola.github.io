[
  {
    "id": 1,
    "company": "NVIDIA",
    "author": "Hao Hoang",
    "title": "Attach a Random Linear Head to Transformer",
    "category": ["Transfer Learning", "NLP"],
    "trap": "Unfreeze all layers and start backprop immediately.",
    "why_trap": "The new head is initialized with random noise. Backpropagating this 'mathematical garbage' destroys the robust features of the pre-trained backbone (The Gradient Shockwave).",
    "solution": "<strong>Linear Probing, then Fine-Tuning.</strong><br>1. Freeze backbone, train only the head (The Shield).<br>2. Unfreeze backbone.<br>3. Lower learning rate and fine-tune full model.",
    "hired_answer": "\"Never fine-tune a backbone against a random head. We must align the head to the existing feature space before we allow the feature space to shift.\""
  },
  {
    "id": 2,
    "company": "DeepMind",
    "author": "Hao Hoang",
    "title": "Predict First 1000 Steps (Adam & Transformers)",
    "category": ["Optimization", "Transformers"],
    "trap": "Predicting convergence with constant Learning Rate (1e-3).",
    "why_trap": "Transformers lack inductive bias at initialization. High LR on random weights causes gradient variance explosion, pushing parameters into unrecoverable loss landscapes.",
    "solution": "<strong>Linear Warm-up Schedule.</strong><br>Start LR at 0.0, linearly increase over ~4,000 steps to let curvature stabilize, then decay.",
    "hired_answer": "\"Transformers don't tolerate high learning rates at initialization due to high gradient variance. I would implement a linear warm-up from 0 to d_model^(-0.5) to let the curvature stabilize.\""
  },
  {
    "id": 3,
    "company": "OpenAI",
    "author": "Hao Hoang",
    "title": "Greedy Search for Low-Latency Trivia Bot",
    "category": ["Search Algorithms", "Inference"],
    "trap": "Yes, use Greedy Search because it's O(N) and fast.",
    "why_trap": "Confusing Local Optima with Global Optima. Greedy search locks into high-probability prefixes (like 'New...') that might lead to dead ends, missing the correct answer ('Pennsylvania').",
    "solution": "<strong>Beam Search (Narrow Width).</strong><br>Keep top k=3 to 5 paths active. Allows the model to 'change its mind' if a path's probability drops later.",
    "hired_answer": "\"Never trade global coherence for local probability. Even for low-latency tasks, a Beam Width of 3-5 is the minimum requirement to prevent the model from locking itself into high-probability errors.\""
  },
  {
    "id": 4,
    "company": "Walmart",
    "author": "Hao Hoang",
    "title": "5 Years of Transaction History for Next Month's Prediction",
    "category": ["Data Engineering", "System Design"],
    "trap": "Ingest the whole 5-year history and train a massive XGBoost model.",
    "why_trap": "<strong>The Silent Graveyard Effect (Survivorship Bias).</strong><br>Historical logs exclude churned users. The model over-indexes on loyalists and fails to spot at-risk users.",
    "solution": "<strong>Time-Travel Feature Engineering.</strong><br>Reconstruct historical states (e.g., snapshot at T-2 years), label based on future behavior, and force 'failures' back into training.",
    "hired_answer": "\"Historical logs suffer from severe survivorship bias. We must explicitly reconstruct historical states to include the 'ghosts'—users who churned—otherwise, the model will never learn to spot an exit risk.\""
  },
  {
    "id": 5,
    "company": "DeepMind",
    "author": "Hao Hoang",
    "title": "Adding 90 Languages to a 1B Parameter Model",
    "category": ["NLP", "Scaling"],
    "trap": "It will improve English metrics via transfer learning.",
    "why_trap": "<strong>Curse of Multilinguality.</strong><br>With a fixed parameter budget, adding languages is a zero-sum game. High-resource languages get diluted as capacity is spread too thin.",
    "solution": "<strong>The Bottleneck Injection.</strong><br>Freeze the backbone. Inject lightweight <em>Adapter Modules</em> for specific languages to reserve capacity without retraining the dense model.",
    "hired_answer": "\"Don't dilute the weights. Freeze the backbone for shared transfer, and use Language-Specific Adapters to reserve capacity for high-resource markets.\""
  },
  {
    "id": 6,
    "company": "OpenAI",
    "author": "Hao Hoang",
    "title": "Implementing Softmax Regression",
    "category": ["Optimization", "Coding"],
    "trap": "Writing clean, modular code: `probs = softmax(logits); loss = -log(probs)`.",
    "why_trap": "<strong>IEEE 754 Floating-point Hell.</strong><br>Tiny probabilities round to 0.0. `log(0.0)` is `-inf`. Backprop becomes `NaN`. The model explodes.",
    "solution": "<strong>LogSumExp Fusion.</strong><br>Use `cross_entropy_with_logits`. It calculates loss directly from raw logits, bypassing explicit probability calculation to prevent underflow.",
    "hired_answer": "\"I don't separate Softmax and Log. I use cross_entropy_with_logits. It fuses the operations to guarantee numerical convexity and prevents NaN explosions during mixed-precision training.\""
  },
  {
    "id": 7,
    "company": "General",
    "author": "Hao Hoang",
    "title": "Feature Pipeline 'Adds Information'",
    "category": ["Feature Engineering", "Information Theory"],
    "trap": "Believing that running text through NER/POS taggers adds information.",
    "why_trap": "<strong>Data Processing Inequality (DPI).</strong><br>Deterministic processing cannot add new information. H(Y|X) is zero. You are just restructuring bits.",
    "solution": "<strong>Distillation, not Enrichment.</strong><br>Focus on stripping nuisance variables to maximize V-Information (computationally useful information).",
    "hired_answer": "\"Stop thinking about Enrichment. Start thinking about Distillation. Good feature engineering isn't about piling on columns; it's about stripping away noise so the model can find the signal.\""
  },
  {
    "id": 8,
    "company": "OpenAI",
    "author": "Hao Hoang",
    "title": "LoRA Rank Increase (8 to 256) Flatlines Loss",
    "category": ["Fine-Tuning", "LoRA"],
    "trap": "It's overfitting; rank 256 is too high.",
    "why_trap": "<strong>Vanishing Update Paradox.</strong><br>Standard LoRA scales updates by `alpha/r`. As `r` grows, the update factor shrinks, forcing gradients toward zero.",
    "solution": "<strong>Rank-Stabilized LoRA (rsLoRA).</strong><br>Scale updates by `alpha/sqrt(r)` to stabilize gradient norms across all ranks.",
    "hired_answer": "\"Standard LoRA scaling penalizes high ranks. I would switch to rsLoRA to correct the gradient collapse. This lets us scale r indefinitely to capture domain complexity.\""
  },
  {
    "id": 9,
    "company": "Google",
    "author": "Hao Hoang",
    "title": "Scaling ML Service to 50k TPS (Postgres Source)",
    "category": ["System Design", "Infrastructure"],
    "trap": "Optimizing the SQL (Read Replicas, Sharding, Caching).",
    "why_trap": "<strong>Database-as-Queue Anti-Pattern.</strong><br>Reading from disk-based persistence of upstream services creates lock contention and connection saturation.",
    "solution": "<strong>Transport Decoupling.</strong><br>Use an in-memory event broker (Kafka/RabbitMQ). Upstreams fire events; ML service reads from the stream in memory.",
    "hired_answer": "\"Databases are for Persistence (record of truth). Brokers are for Transport (hand-off). Never force your real-time inference path to wait on a disk write.\""
  },
  {
    "id": 10,
    "company": "OpenAI",
    "author": "Hao Hoang",
    "title": "Class Imbalance (50k City vs 45 Deer)",
    "category": ["Computer Vision", "Data Engineering"],
    "trap": "Ramp up standard data augmentation (rotation, flips, color jitter).",
    "why_trap": "<strong>Geometric Invariance vs Diversity.</strong><br>A rotated deer is the same deer. You aren't adding semantic information, just noise.",
    "solution": "<strong>Semantic Synthesis.</strong><br>Use generative models (Inpainting/Stable Diffusion) to synthesize <em>new</em> deer in existing contexts.",
    "hired_answer": "\"Geometric augmentation solves for robust features. Generative synthesis solves for distribution shifts. In a data-starved regime, we need synthesis, not just rotation.\""
  },
  {
    "id": 11,
    "company": "DeepMind",
    "author": "Hao Hoang",
    "title": "Handling Missing Values (Median Fill)",
    "category": ["Data Engineering", "Methodology"],
    "trap": "Calculate median of the column and fill empty cells.",
    "why_trap": "<strong>Data Leakage.</strong><br>Calculating median on the whole dataset snoops on the test set. The model learns a distribution it shouldn't see.",
    "solution": "<strong>Time Machine Pipeline.</strong><br>Split Data First. Fit imputer on Train. Transform both Train and Test using Train statistics.",
    "hired_answer": "\"I never calculate global statistics. I fit my imputers on the training set and apply that transformation to the test set to prevent data leakage.\""
  },
  {
    "id": 12,
    "company": "DeepMind",
    "author": "Hao Hoang",
    "title": "Automated Trigger for Retraining (Feature Drift)",
    "category": ["ML Ops", "Statistics"],
    "trap": "Run KS Test (p-value < 0.05).",
    "why_trap": "<strong>The P-Value Mirage.</strong><br>At large N (e.g., 1M), KS test is hypersensitive. Meaningless shifts yield tiny p-values, causing constant retraining.",
    "solution": "<strong>The Magnitude Metric.</strong><br>Measure distance, not probability. Use Wasserstein Distance or PSI. Trigger only when distance correlates with loss degradation.",
    "hired_answer": "\"At production scale, p-values are useless noise. I monitor distributional distance (like Wasserstein or PSI) and trigger retraining only when that distance crosses a threshold that historically impacts loss.\""
  },
  {
    "id": 13,
    "company": "OpenAI",
    "author": "Hao Hoang",
    "title": "Edge Model Quantization vs Robustness",
    "category": ["Edge AI", "Quantization"],
    "trap": "Avoid quantization (keep Float32) to preserve stability against noise.",
    "why_trap": "<strong>Conflating Precision with Robustness.</strong><br>High precision captures noise perfectly. It makes the model hyper-sensitive.",
    "solution": "<strong>The Bit-Depth Barrier.</strong><br>Quantization acts as a low-pass filter (The Rounding Shield). Combine with Lipschitz Regularization.",
    "hired_answer": "\"Precision is not robustness. By combining Lipschitz constraints with Int8 quantization, we turn discretization error into a defensive feature, stripping out small-scale adversarial noise.\""
  },
  {
    "id": 14,
    "company": "Google for Health",
    "author": "Hao Hoang",
    "title": "Gallbladder Segmentation (99.2% Accuracy)",
    "category": ["Computer Vision", "Safety"],
    "trap": "Deploy it. The metrics are perfect.",
    "why_trap": "<strong>Clever Hans Effect.</strong><br>The model learned artifacts (e.g., a surgical tool), not anatomy. It fails when the tool changes.",
    "solution": "<strong>The Black Patch Protocol.</strong><br>Adversarial Stress Testing. Black out the organ; if the model still predicts it, it's looking at the background.",
    "hired_answer": "\"Test set metrics measure correlation, not causality. I validate focus. If the model predicts the target when the target is invisible, it's not a model, it's a random number generator.\""
  },
  {
    "id": 15,
    "company": "OpenAI",
    "author": "Hao Hoang",
    "title": "Transformer Loss is Flat (New Implementation)",
    "category": ["Debugging", "Transformers"],
    "trap": "Start tuning hyperparameters (LR, Optimizer, Normalization).",
    "why_trap": "<strong>Solving for performance instead of correctness.</strong><br>Bugs in ML are silent. Tuning parameters on a broken implementation is useless.",
    "solution": "<strong>The Single-Batch Overfit.</strong><br>Strip regularization. Train on exactly one batch. Loss must hit 0.00. If not, the code is broken.",
    "hired_answer": "\"I don't tune a model until I prove it can learn. I strip regularization and force the model to overfit a single batch. If it can't memorize, it will never generalize.\""
  },
  {
    "id": 16,
    "company": "Meta",
    "author": "Hao Hoang",
    "title": "Adapt Llama-3 70B to Medical Domain (GPU Constrained)",
    "category": ["Fine-Tuning", "LoRA"],
    "trap": "Use standard LoRA.",
    "why_trap": "<strong>Style vs Substance.</strong><br>Standard LoRA is good for instruction tuning (style), but poor for injecting new knowledge (substance) due to limited rank updates.",
    "solution": "<strong>The Hyper-Adaptation Trifecta.</strong><br>1. RS-LoRA (Rank-Stabilized).<br>2. LoftQ Initialization.<br>3. Differential Learning Rates.",
    "hired_answer": "\"Standard LoRA is for behavior. The Trifecta is for knowledge. I use RS-LoRA and Differential Learning Rates to match full fine-tuning performance on a budget.\""
  },
  {
    "id": 17,
    "company": "OpenAI",
    "author": "Hao Hoang",
    "title": "GDPR Deletion & New Class Learning",
    "category": ["Continual Learning", "Safety"],
    "trap": "Fine-tune on new data (perhaps with lower LR).",
    "why_trap": "<strong>Catastrophic Forgetting.</strong><br>Fine-tuning exclusively on new data overwrites weights critical for old tasks.",
    "solution": "<strong>Elastic Weight Consolidation (EWC).</strong><br>Calculate Fisher Information Matrix to identify critical weights. Penalize changes to these weights in the loss function.",
    "hired_answer": "\"I wouldn't just fine-tune. I would implement Elastic Weight Consolidation (EWC) to penalize changes to high-importance weights, allowing us to learn the new class while locking in prior knowledge.\""
  },
  {
    "id": 18,
    "company": "Google",
    "author": "Hao Hoang",
    "title": "Fraud Detection Model (0.98 ROC AUC)",
    "category": ["Metrics", "Fraud Detection"],
    "trap": "Deploy. 0.98 is phenomenal.",
    "why_trap": "<strong>The Majority Mirage.</strong><br>ROC is addicted to True Negatives. In high imbalance, massive legitimate traffic dilutes errors, hiding poor recall.",
    "solution": "<strong>Precision-Recall Curve (AUPRC).</strong><br>Switch metrics. PR Curve ignores True Negatives and focuses on minority class performance.",
    "hired_answer": "\"ROC scores are optimistic proxies in high-imbalance domains. I ignore ROC and optimize for Area Under the Precision-Recall Curve (AUPRC) to ensure we aren't hiding poor recall.\""
  },
  {
    "id": 19,
    "company": "LinkedIn",
    "author": "Hao Hoang",
    "title": "Real-time Social Graph Recommendations",
    "category": ["System Design", "Recommender Systems"],
    "trap": "Use a Graph Neural Network (GNN).",
    "why_trap": "<strong>The Complexity Cliff.</strong><br>GNNs have massive latency and infrastructure costs. Un-shippable for 50k TPS real-time needs.",
    "solution": "<strong>Value Hierarchy.</strong><br>1. Heuristics (Mutual Friends).<br>2. Tree-based models (XGBoost) with graph features.<br>3. GNN only for edge cases.",
    "hired_answer": "\"I will earn the right to build a GNN only after a heuristic baseline and a lightweight tree-based model stop delivering ROI. We don't burn GPU credits on problems solvable with a GROUP BY.\""
  },
  {
    "id": 20,
    "company": "General",
    "author": "Hao Hoang",
    "title": "High CTR but Retention Tanking",
    "category": ["Recommender Systems", "Product"],
    "trap": "Assume content quality dropped or model is learning well (High Engagement = High Learning).",
    "why_trap": "<strong>The Echo Chamber Collapse.</strong><br>Optimizing for availability, not preference. The model predicts its own past decisions (Selection Bias), narrowing content and boring users.",
    "solution": "<strong>Counterfactual Evaluation.</strong><br>1. Exploration (epsilon-greedy).<br>2. Inverse Propensity Weighting (IPW) to debias clicks.",
    "hired_answer": "\"A model that only learns from its own uncorrected predictions is destined to collapse. We don't just optimize for clicks today; we optimize for the informational value of the training data.\""
  },
  {
    "id": 21,
    "company": "TikTok",
    "author": "Hao Hoang",
    "title": "Retraining Frequency for Core RecSys",
    "category": ["System Design", "ML Ops"],
    "trap": "Retrain weekly or nightly.",
    "why_trap": "<strong>The 10-Minute Horizon.</strong><br>User intent is not static. If a trend explodes at 2 PM, a model trained at 4 AM is statistically blind.",
    "solution": "<strong>Online/Incremental Learning.</strong><br>Update model weights every ~10 minutes using micro-batches of user signals.",
    "hired_answer": "\"In high-velocity consumer apps, Freshness is not a maintenance metric - it is a performance feature. We don't optimize for 'stable' weights; we optimize for 'live' distribution shifts.\""
  },
  {
    "id": 22,
    "company": "DoorDash",
    "author": "Hao Hoang",
    "title": "Balancing Clicks vs High-Commission Orders",
    "category": ["System Design", "Business Logic"],
    "trap": "Bake the trade-off into the Loss Function (alpha * LogLoss + beta * MSE).",
    "why_trap": "<strong>Coupling Engineering to Business OKRs.</strong><br>Hard-coding business logic into weights requires retraining for every business decision change.",
    "solution": "<strong>The Decoupled Objective Protocol.</strong><br>Train model purely on probabilities. Apply business logic (weights) dynamically at the <em>Serving Layer</em>.",
    "hired_answer": "\"We don't use ML to learn the trade-off. We use ML to learn the probabilities, and we define the trade-off dynamically at runtime.\""
  },
  {
    "id": 23,
    "company": "Twitter",
    "author": "Hao Hoang",
    "title": "Statistically Representative Sample of Infinite Stream",
    "category": ["System Design", "Algorithms"],
    "trap": "Buffer last hour or flip a coin (Bernoulli sampling).",
    "why_trap": "<strong>Memory Wall & Probability Bias.</strong><br>Stream is infinite; memory is finite. Fixed probability yields variable sample size.",
    "solution": "<strong>Reservoir Sampling.</strong><br>Maintain fixed buffer size k. For nth item, replace element at random index with probability k/n.",
    "hired_answer": "\"I will use Reservoir Sampling to maintain a fixed-size buffer of k items. This guarantees uniform probability over an infinite stream with O(1) space complexity.\""
  },
  {
    "id": 24,
    "company": "DeepMind",
    "author": "Hao Hoang",
    "title": "1:1000 Class Imbalance (Fraud)",
    "category": ["Loss Functions", "Fraud Detection"],
    "trap": "Aggressive oversampling or higher class weights.",
    "why_trap": "<strong>Gradient Drowning.</strong><br>Easy negatives (obvious legit transactions) dominate the gradient sum, drowning out signal from hard positives.",
    "solution": "<strong>Focal Loss.</strong><br>Down-weight easy examples. Force model to focus 100% of gradient budget on hard edge cases.",
    "hired_answer": "\"Weighted Cross-Entropy balances counts. Focal Loss balances hardness. I would implement Focal Loss to down-weight the easy negatives that are currently dominating the gradient.\""
  },
  {
    "id": 25,
    "company": "General",
    "author": "Hao Hoang",
    "title": "2000 Labeled Samples of Mouse Movements",
    "category": ["Data Scarcity", "Architecture"],
    "trap": "Use LSTM/GRU/Transformer (Sequence Models).",
    "why_trap": "<strong>Overfitting to Noise.</strong><br>With 2k samples, complex sequence models memorize noise. It's a data scarcity problem.",
    "solution": "<strong>The Splunk Approach (Visual Encoding).</strong><br>Plot coordinates as images. Use Transfer Learning from a pre-trained CNN (ResNet).",
    "hired_answer": "\"I have a data scarcity problem, not a modeling problem. I would render the temporal sequences as static images. This allows me to leverage Transfer Learning from pre-trained CNNs.\""
  },
  {
    "id": 26,
    "company": "OpenAI",
    "author": "Hao Hoang",
    "title": "Auto-bidder for Home Sales (RMSE 1.5%)",
    "category": ["Financial ML", "Loss Functions"],
    "trap": "Deploy with confidence threshold.",
    "why_trap": "<strong>Adversarial Selection Bias.</strong><br>Symmetric loss treats overpayment and underpayment equally. You win bids only when you overestimate, acquiring toxic assets.",
    "solution": "<strong>PnL-Weighted Asymmetric Loss (Quantile Loss).</strong><br>Penalize overestimation 100x more than underestimation.",
    "hired_answer": "\"I would refuse to deploy a model trained on symmetric loss. I would retrain using an Asymmetric Loss Function where overestimation carries a massive penalty compared to underestimation.\""
  },
  {
    "id": 27,
    "company": "Anthropic",
    "author": "Hao Hoang",
    "title": "Leaderboard Illusion (Boosting ELO)",
    "category": ["LLM Evaluation", "Product"],
    "trap": "Improve reasoning on hard math benchmarks.",
    "why_trap": "<strong>Goodhart's Law / Leaderboard Illusion.</strong><br>Arena ELO favors verbosity and 'quizzing' prompts, not real-world utility.",
    "solution": "<strong>Golden Set Optimization.</strong><br>Ignore vanity metrics. Optimize for a Golden Set derived from actual paying customer logs.",
    "hired_answer": "\"I won't degrade our model to chase a vanity metric. ELO measures vibes and verbosity, not utility. We should optimize for a Golden Set derived from our actual paying customers.\""
  },
  {
    "id": 28,
    "company": "DeepMind",
    "author": "Hao Hoang",
    "title": "Backprop Cost Calculation (Intern Estimate)",
    "category": ["Training Dynamics", "Compute"],
    "trap": "Estimate Backprop cost as equal to Forward pass (2x total).",
    "why_trap": "<strong>Backprop Asymmetry Principle.</strong><br>Backward pass computes TWO gradients (weights and input activations). It is 2x the cost of forward pass.",
    "solution": "<strong>The 1:2 Ratio.</strong><br>Total FLOPs = 6 * N * D (2 for forward, 4 for backward).",
    "hired_answer": "\"The backward pass isn't symmetrical; it's dual-purpose. We must compute gradients for both weights and inputs. This makes backward pass 2x the cost of forward, meaning true FLOP count is 6ND.\""
  },
  {
    "id": 29,
    "company": "OpenAI",
    "author": "Hao Hoang",
    "title": "OOM Errors: Casting to bfloat16",
    "category": ["Training Dynamics", "Precision"],
    "trap": "Cast everything (including optimizer state) to bfloat16.",
    "why_trap": "<strong>The Mantissa Trap.</strong><br>bfloat16 has narrow precision. Tiny gradient updates round to zero when added to model parameters.",
    "solution": "<strong>Mixed Precision Training.</strong><br>Compute in bfloat16. Store Master Weights and Optimizer States in FP32.",
    "hired_answer": "\"You can compute in low precision, but you must store in high precision. Casting optimizer states to bfloat16 causes 'swallowing' of small gradient updates. You need FP32 Master Weights.\""
  },
  {
    "id": 30,
    "company": "NVIDIA",
    "author": "Hao Hoang",
    "title": "1000x Speedup on Matrix Mult (Fake Benchmark)",
    "category": ["CUDA/GPU", "Profiling"],
    "trap": "Suggest higher precision timers or cache warmup.",
    "why_trap": "<strong>Asynchronous Execution Trap.</strong><br>CPU launches kernel and moves on. The timer measured kernel launch latency, not execution.",
    "solution": "<strong>Synchronization Barrier.</strong><br>Must use `torch.cuda.synchronize()` before stopping the timer.",
    "hired_answer": "\"The benchmark is fake because the CPU didn't wait for the GPU. You must insert a Synchronization Barrier before stopping the timer.\""
  },
  {
    "id": 31,
    "company": "DeepMind",
    "author": "Hao Hoang",
    "title": "DeepSeek GRPO CoT Explosion",
    "category": ["RLHF", "LLM Training"],
    "trap": "It's emergent reasoning behavior.",
    "why_trap": "<strong>Reward Hacking.</strong><br>Length normalization incentives the model to dilute negative rewards by generating massive amounts of fluff text.",
    "solution": "<strong>Fix the RL Objective.</strong><br>Remove length normalization or replace with explicit shortness penalty for correct answers.",
    "hired_answer": "\"This isn't emergence, it's reward hacking. The length_normalization is incentivizing the model to dilute negative rewards by exploding sequence length. We must modify the RL objective.\""
  },
  {
    "id": 32,
    "company": "Perplexity",
    "author": "Hao Hoang",
    "title": "Fine-tuning on Random User Prompts",
    "category": ["Data Curation", "Product"],
    "trap": "Data is noisy/PII.",
    "why_trap": "<strong>Distribution Mismatch (Quizzing vs Asking).</strong><br>90% of traffic is low-value 'quizzing'. Training on this makes the model a toy.",
    "solution": "<strong>Intent Filtering.</strong><br>Build a curation pipeline to filter for high-intent 'Asking' prompts.",
    "hired_answer": "\"The goal isn't to model the average user; it's to model the ideal user. I'd build a data curation pipeline that uses classifiers to aggressively filter for high-intent 'asking' prompts.\""
  },
  {
    "id": 33,
    "company": "OpenAI",
    "author": "Hao Hoang",
    "title": "Downloading Common Crawl (1 Day Budget)",
    "category": ["Data Engineering", "LLM Training"],
    "trap": "Focus on regex filtering/deduping.",
    "why_trap": "<strong>The Parser Problem.</strong><br>Frontier labs 'manufacture' data. Parsing, classifier-based filtering, and semantic deduping are massive engineering tasks.",
    "solution": "<strong>Data Curation Engine.</strong><br>Treat data prep as a distributed systems problem involving parsing, scoring classifiers, and massive deduping.",
    "hired_answer": "\"We don't 'find' good data; we build it. The data pipeline—from HTML parsing to classifier-based filtering—is the most valuable and secretive part of the stack.\""
  },
  {
    "id": 34,
    "company": "Anthropic",
    "author": "Hao Hoang",
    "title": "8x Batch Size for Throughput",
    "category": ["Inference", "System Optimization"],
    "trap": "It increases latency just because batch is bigger.",
    "why_trap": "<strong>Throughput vs Latency Curve.</strong><br>Increasing batch size improves throughput only up to a point. Beyond the 'hump', memory I/O overhead increases latency without throughput gain.",
    "solution": "<strong>Optimize for Peak Throughput.</strong><br>Increase batch size only up to the peak of the throughput-latency curve.",
    "hired_answer": "\"Throughput is a cost metric, but per-token latency is a product metric. We can increase batch size, but only up to the peak of the throughput-latency curve.\""
  },
  {
    "id": 35,
    "company": "OpenAI",
    "author": "Hao Hoang",
    "title": "SwiGLU vs ReLU (Theoretical Proof)",
    "category": ["Research Strategy", "Architecture"],
    "trap": "Encourage theoretical research.",
    "why_trap": "<strong>Divine Benevolence.</strong><br>Large-scale AI is empirical. We often don't know 'why' theoretically, but ablation studies prove 'that' it works.",
    "solution": "<strong>Empirical Lift over Theory.</strong><br>Trust the ablations. Ship based on evidence, write theory later.",
    "hired_answer": "\"I stop the theoretical exploration immediately. In large-scale training, we follow the empirical lift. Our ablation studies are our proof.\""
  },
  {
    "id": 36,
    "company": "DeepMind",
    "author": "Hao Hoang",
    "title": "RoPE Implementation (Added to Embeddings)",
    "category": ["Transformers", "Architecture"],
    "trap": "Position info lost in residual stream.",
    "why_trap": "<strong>Linear Projections Destroy Rotation.</strong><br>Adding RoPE to input means W_q and W_k projections destroy the rotational properties.",
    "solution": "<strong>Apply in Attention Block.</strong><br>Apply RoPE to Q and K vectors <em>after</em> projection, just before dot product.",
    "hired_answer": "\"RoPE isn't an 'embedding' you add at the input. It's a rotational transformation you must apply to Q and K vectors inside every attention layer.\""
  },
  {
    "id": 37,
    "company": "Meta",
    "author": "Hao Hoang",
    "title": "MHA vs GQA Inference Speed",
    "category": ["Inference", "Architecture"],
    "trap": "FLOPs are identical, so speed is same.",
    "why_trap": "<strong>Memory-Bound Inference.</strong><br>Inference is bound by memory bandwidth (reading KV cache), not compute. MHA has massive KV cache.",
    "solution": "<strong>Grouped Query Attention (GQA).</strong><br>Reduces KV heads, shrinking cache size and HBM read pressure.",
    "hired_answer": "\"FLOPs are irrelevant. Inference is memory-bandwidth bound. GQA is faster because it shrinks the KV Cache, cutting HBM read pressure.\""
  },
  {
    "id": 38,
    "company": "Google",
    "author": "Hao Hoang",
    "title": "TTFT vs Throughput Workloads",
    "category": ["Inference", "System Design"],
    "trap": "Change batch size only.",
    "why_trap": "<strong>Prefill (Compute) vs Generation (Memory).</strong><br>Chatbot TTFT is compute-bound (prefill). Batch job is memory-bound (generation).",
    "solution": "<strong>Dual-Optimization.</strong><br>Optimize prefill for compute (Chatbot). Optimize generation for memory bandwidth saturation (Batch).",
    "hired_answer": "\"The chatbot's TTFT is dominated by the compute-bound prefill. The batch job is limited by the memory-bound generation. I'd optimize separately for these distinct bottlenecks.\""
  },
  {
    "id": 39,
    "company": "Anthropic",
    "author": "Hao Hoang",
    "title": "Weight Decay on 1 Epoch Training",
    "category": ["Optimization", "Training Dynamics"],
    "trap": "It's for regularization (prevent overfitting).",
    "why_trap": "<strong>Optimization Dynamics.</strong><br>Weight decay interacts with the cosine schedule to manipulate optimization path, not just regularize.",
    "solution": "<strong>Tail-End Dynamics.</strong><br>Allows model to converge to better final loss as LR decays.",
    "hired_answer": "\"We're not using it for regularization. We're using it to manage optimizer dynamics. It interacts with the cosine schedule to allow better convergence at the tail end.\""
  },
  {
    "id": 40,
    "company": "DeepMind",
    "author": "Hao Hoang",
    "title": "MoE Validation Loss Flatline",
    "category": ["MoE", "Training Dynamics"],
    "trap": "High LR or data bug.",
    "why_trap": "<strong>Router Collapse.</strong><br>Router sends all tokens to a few 'hero' experts. Others starve.",
    "solution": "<strong>Auxiliary Balancing Loss.</strong><br>Penalize router for imbalance to force expert utilization.",
    "hired_answer": "\"The most likely cause is router collapse leading to expert starvation. The fix is to tune the auxiliary balancing loss to force even token distribution.\""
  },
  {
    "id": 41,
    "company": "OpenAI",
    "author": "Hao Hoang",
    "title": "RL Fine-Tuning Makes Model 'Dumber'",
    "category": ["RLHF", "Alignment"],
    "trap": "Catastrophic forgetting.",
    "why_trap": "<strong>Alignment Tax / Reward Hacking.</strong><br>Policy exploits the Reward Model, diverging from general knowledge.",
    "solution": "<strong>KL Divergence Penalty.</strong><br>Penalize divergence from a frozen reference model (SFT).",
    "hired_answer": "\"This is the alignment tax. I would add a KL divergence penalty between the active policy and a frozen reference model to regularize the policy.\""
  },
  {
    "id": 42,
    "company": "DeepMind",
    "author": "Hao Hoang",
    "title": "Reasoning Model Over-thinking (2+2)",
    "category": ["SFT", "Inference"],
    "trap": "Train two models (small and large) and route.",
    "why_trap": "<strong>Routing Complexity.</strong><br>Adds maintenance nightmare and infrastructure cost.",
    "solution": "<strong>Thinking Mode Fusion.</strong><br>Train one model with `<think>` and `<no_think>` tags. Control behavior via prompt.",
    "hired_answer": "\"Don't build a multi-model routing system. Use Thinking Mode Fusion to teach one model to decide when to think and when to act via control tokens.\""
  },
  {
    "id": 43,
    "company": "OpenAI",
    "author": "Hao Hoang",
    "title": "10M Context Window (Attention O(N^2))",
    "category": ["Attention", "CUDA"],
    "trap": "Compute is the bottleneck.",
    "why_trap": "<strong>Memory I/O Bottleneck.</strong><br>Standard attention reads/writes massive matrices to HBM. GPU starves waiting for data.",
    "solution": "<strong>FlashAttention (Tiling & Fusion).</strong><br>Compute in SRAM using tiling. Avoid writing intermediate matrices to HBM.",
    "hired_answer": "\"Standard attention is memory-bound. FlashAttention fixes this by using tiling and kernel fusion to keep operations in fast SRAM, making it compute-bound again.\""
  },
  {
    "id": 44,
    "company": "Anthropic",
    "author": "Hao Hoang",
    "title": "95% MMLU Score Validation",
    "category": ["Evaluation", "Data Contamination"],
    "trap": "Check for train-test overlap (n-gram).",
    "why_trap": "<strong>Semantic Contamination.</strong><br>Paraphrased questions, translations, and guides won't be caught by n-gram checks.",
    "solution": "<strong>Semantic Contamination Audit.</strong><br>Use embedding-based near-duplicate detection.",
    "hired_answer": "\"I don't trust the 95% until I verify we didn't memorize the test. I'm immediately launching a semantic contamination audit using embedding-based detection.\""
  },
  {
    "id": 45,
    "company": "DeepMind",
    "author": "Hao Hoang",
    "title": "Scaling 1B to 70B (Exploding Gradients)",
    "category": ["Scaling Laws", "Architecture"],
    "trap": "Clip gradients, lower LR manually.",
    "why_trap": "<strong>Standard Parameterization Failure.</strong><br>Optimal LR shifts as model width changes.",
    "solution": "<strong>Maximum Update Parameterization (MUP).</strong><br>Scale initialization and LR rules so optimal hyperparameters are scale-invariant.",
    "hired_answer": "\"With Standard Parameterization, LR is unstable at scale. With MUP, I find the optimal LR on a proxy model and it remains optimal at 70B.\""
  },
  {
    "id": 46,
    "company": "OpenAI",
    "author": "Hao Hoang",
    "title": "2x Speedup on Llama 3 (Lossless)",
    "category": ["Inference", "Optimization"],
    "trap": "Better batching / PagedAttention.",
    "why_trap": "<strong>Autoregressive is Memory Bound.</strong><br>Generating tokens is slow. Checking tokens is fast.",
    "solution": "<strong>Speculative Decoding.</strong><br>Use small draft model to propose tokens. Verify in parallel with large model.",
    "hired_answer": "\"I'd implement Speculative Decoding. We exploit the asymmetry between fast verification and slow generation to losslessly accelerate inference.\""
  },
  {
    "id": 47,
    "company": "Anthropic",
    "author": "Hao Hoang",
    "title": "Compute Budget: Larger Model vs More Data?",
    "category": ["Scaling Laws", "Strategy"],
    "trap": "Guessing (e.g., Smaller model on more data).",
    "why_trap": "<strong>Guessing wastes budget.</strong><br>At scale, decisions must be data-driven.",
    "solution": "<strong>Scaling Laws Experiment.</strong><br>Use 10% of budget to run small experiments, fit loss curve, and predict optimal allocation.",
    "hired_answer": "\"I wouldn't guess. I'd use 10% of the budget to run small-scale experiments, fit a scaling law, and optimize the allocation based on the predictive model.\""
  },
  {
    "id": 48,
    "company": "Anthropic",
    "author": "Hao Hoang",
    "title": "Standard Tokenizer on Medical Domain",
    "category": ["NLP", "Efficiency"],
    "trap": "It will learn subwords eventually.",
    "why_trap": "<strong>Sequence Length Bloat.</strong><br>General tokenizers shatter domain terms into many pieces, exploding sequence length and O(N^2) attention cost.",
    "solution": "<strong>Domain-Specific Tokenizer.</strong><br>Train a new tokenizer on the domain corpus.",
    "hired_answer": "\"Re-using the tokenizer is a false economy. It causes sequence length bloat. I would train a domain-specific tokenizer to dramatically reduce the token count and training cost.\""
  },
  {
    "id": 49,
    "company": "Google",
    "author": "Hao Hoang",
    "title": "Post-Norm vs Pre-Norm (Deep Models)",
    "category": ["Architecture", "Transformers"],
    "trap": "Post-norm is less stable; use warm-up.",
    "why_trap": "<strong>Poisoned Residual Stream.</strong><br>Post-norm places LayerNorm on the residual path, warping gradients in deep networks.",
    "solution": "<strong>Pre-Norm.</strong><br>Normalize input to sub-layers. Keep residual stream clean.",
    "hired_answer": "\"Post-norm breaks the clean identity path of the residual stream. Pre-norm solves this by normalizing inputs to sub-layers, preserving the 'gradient highway'.\""
  },
  {
    "id": 50,
    "company": "OpenAI",
    "author": "Hao Hoang",
    "title": "Byte-based vs BPE Tokenizer",
    "category": ["NLP", "Compute"],
    "trap": "BPE captures semantics.",
    "why_trap": "<strong>Compute Cost (FLOPs).</strong><br>Byte tokenizer has 1.0 compression. BPE compresses 4x. Bytes explode sequence length and attention cost.",
    "solution": "<strong>BPE for Efficiency.</strong><br>Use BPE to reduce sequence length.",
    "hired_answer": "\"BPE is an efficiency hack. It dramatically reduces sequence length, which is the biggest bottleneck for Transformer O(N^2) compute cost.\""
  },
  {
    "id": 51,
    "company": "Meta",
    "author": "Hao Hoang",
    "title": "KV Caching Bottleneck",
    "category": ["Inference", "Memory Management"],
    "trap": "It reduces compute from O(N^2) to O(N).",
    "why_trap": "<strong>VRAM Fragmentation.</strong><br>Naive KV caching pre-allocates contiguous memory, wasting massive amounts of VRAM.",
    "solution": "<strong>PagedAttention (Dynamic Allocation).</strong><br>Manage cache like OS virtual memory (pages).",
    "hired_answer": "\"KV Caching shifts bottleneck to VRAM capacity. The solution is dynamic cache management (PagedAttention) to eliminate fragmentation and maximize batch size.\""
  }
]